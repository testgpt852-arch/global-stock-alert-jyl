import aiohttp
import asyncio
import logging
from datetime import datetime, timedelta
from collections import Counter
import re
from config import Config

logger = logging.getLogger(__name__)

class SocialScanner:
    def __init__(self):
        self.reddit_base = "https://www.reddit.com"
        self.last_posts = set()
        
        # [í•µì‹¬] WSBì—ì„œ ìì£¼ ì–¸ê¸‰ë˜ëŠ” ì¸ê¸° ì¢…ëª© ë¦¬ìŠ¤íŠ¸ (ë…¸ì´ì¦ˆ ë°©ì§€ìš© í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸)
        # ì´ ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ê±´ $ ì—†ì´ë„ ì¸ì‹, ì—†ëŠ” ê±´ $ê°€ ë¶™ì–´ì•¼ë§Œ ì¸ì‹ ($ABC)
        self.popular_tickers = {
            'NVDA', 'TSLA', 'AAPL', 'AMD', 'MSFT', 'AMZN', 'GOOGL', 'META', 'GME', 'AMC',
            'PLTR', 'SOFI', 'COIN', 'MSTR', 'MARA', 'RIOT', 'HOOD', 'DKNG', 'RIVN', 'LCID',
            'NIO', 'BABA', 'PDD', 'TQQQ', 'SQQQ', 'SOXL', 'SOXS', 'TSLL', 'NVDL', 'BITX',
            'SMCI', 'ARM', 'INTC', 'MU', 'QCOM', 'AVGO', 'NFLX', 'DIS', 'PYPL', 'SQ',
            'AFRM', 'UPST', 'CVNA', 'OPEN', 'Z', 'RDFN', 'PTON', 'ROKU', 'TDOC', 'ZM',
            'SNOW', 'DDOG', 'NET', 'CRWD', 'PANW', 'ZS', 'FTNT', 'NOW', 'CRM', 'ADBE',
            'ORCL', 'IBM', 'CSCO', 'TXN', 'ADI', 'LRCX', 'KLAC', 'AMAT', 'ASML', 'TSM',
            'ON', 'STM', 'WOLF', 'MP', 'ALB', 'LAC', 'LTHM', 'FCX', 'CLF', 'X',
            'NUE', 'STLD', 'AA', 'CENX', 'XOM', 'CVX', 'OXY', 'COP', 'EOG', 'PXD',
            'DVN', 'MRO', 'APA', 'KMI', 'WMB', 'ET', 'MPLX', 'EPD', 'PAA', 'LNG',
            'JPM', 'BAC', 'WFC', 'C', 'GS', 'MS', 'BLK', 'SCHW', 'AXP', 'V',
            'MA', 'PYPL', 'JNJ', 'UNH', 'LLY', 'MRK', 'ABBV', 'PFE', 'BMY', 'AMGN',
            'GILD', 'VRTX', 'REGN', 'MRNA', 'BNTX', 'NVAX', 'SPY', 'QQQ', 'IWM', 'DIA',
            'VIX', 'UVXY', 'UVIX', 'SVIX', 'TLT', 'TMF', 'TMV', 'SH', 'PSQ', 'DJT', 'RDDT'
        }

    async def scan(self):
        """ì†Œì…œ ë¯¸ë””ì–´ íŠ¸ë Œë“œ ìŠ¤ìº”"""
        alerts = []
        
        try:
            # WallStreetBets ìŠ¤ìº”
            wsb_mentions = await self.scan_subreddit('wallstreetbets')
            
            # ì–¸ê¸‰ëŸ‰ ê¸°ì¤€ í•„í„°ë§
            for symbol, count in wsb_mentions.most_common(10):
                if count >= Config.REDDIT_MIN_MENTIONS:
                    logger.info(f"ğŸ”¥ ë ˆë”§ ê¸‰ë“± í¬ì°©: {symbol} ({count}íšŒ)")
                    
                    alert = {
                        'symbol': symbol,
                        'price': 0, # ê°€ê²©ì€ ë‚˜ì¤‘ì— ì±„ì›€
                        'change_percent': 0,
                        'volume': 0,
                        'trigger_type': 'social_trend',
                        'trigger_reason': f'ğŸ”¥ Reddit ì–¸ê¸‰ í­ë°œ ({count}íšŒ/1h)'
                    }
                    alerts.append(alert)
            
        except Exception as e:
            logger.error(f"Social scan error: {e}")
        
        return alerts
    
    async def scan_subreddit(self, subreddit):
        """íŠ¹ì • ì„œë¸Œë ˆë”§ ìŠ¤ìº”"""
        mentions = Counter()
        
        try:
            url = f"{self.reddit_base}/r/{subreddit}/new.json?limit=100" # 100ê°œë¡œ ëŠ˜ë¦¼
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, timeout=10) as response:
                    if response.status != 200:
                        return mentions
                    
                    data = await response.json()
                    posts = data.get('data', {}).get('children', [])
                    
                    # 1ì‹œê°„ ì´ë‚´ ê¸€ë§Œ
                    cutoff_time = datetime.now() - timedelta(hours=1)
                    
                    for post_data in posts:
                        try:
                            post = post_data['data']
                            
                            # ì¤‘ë³µ ì²´í¬
                            if post['id'] in self.last_posts: continue
                            
                            # ì‹œê°„ ì²´í¬
                            created_utc = post.get('created_utc', 0)
                            post_time = datetime.fromtimestamp(created_utc)
                            if post_time < cutoff_time: continue
                            
                            # í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
                            title = post.get('title', '')
                            selftext = post.get('selftext', '')
                            full_text = f"{title} {selftext}"
                            
                            # í‹°ì»¤ ì¶”ì¶œ
                            tickers = self.extract_tickers(full_text)
                            mentions.update(tickers)
                            
                            self.last_posts.add(post['id'])
                            
                        except Exception:
                            continue
                    
                    if len(self.last_posts) > 1000:
                        self.last_posts.clear()
                    
        except Exception as e:
            logger.error(f"Error scanning r/{subreddit}: {e}")
        
        return mentions
    
    def extract_tickers(self, text):
        """[ìˆ˜ì •ë¨] ë…¸ì´ì¦ˆ ì œê±° ê°•í™”"""
        text = text.upper()
        found_tickers = set()
        
        # 1. $ê°€ ë¶™ì€ í‹°ì»¤ ì°¾ê¸° ($TSLA, $AAPL) -> ê°€ì¥ í™•ì‹¤í•¨
        cashtags = re.findall(r'\$([A-Z]{2,5})', text)
        for tag in cashtags:
            # $ê°€ ë¶™ì–´ìˆìœ¼ë©´ ì›¬ë§Œí•˜ë©´ ì¸ì • (ë‹¨, ë„ˆë¬´ í”í•œ ë‹¨ì–´ ì œì™¸)
            if tag not in {'THE', 'FOR', 'NEW', 'USA', 'USD'}:
                found_tickers.add(tag)
        
        # 2. $ ì—†ì´ ë‹¨ì–´ë§Œ ìˆëŠ” ê²½ìš° -> í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ê²ƒë§Œ ì¸ì •
        # (NVDA, GME ê°™ì€ ìœ ëª…í•œ ê±´ $ ì•ˆ ë¶™ì´ê³  ì“°ê¸° ë•Œë¬¸)
        words = re.findall(r'\b([A-Z]{2,5})\b', text)
        for word in words:
            if word in self.popular_tickers:
                found_tickers.add(word)
                
        return list(found_tickers)