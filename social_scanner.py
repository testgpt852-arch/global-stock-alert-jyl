import aiohttp
import asyncio
import logging
from datetime import datetime, timedelta
from collections import Counter
import re

logger = logging.getLogger(__name__)

class SocialScanner:
    def __init__(self):
        self.reddit_base = "https://www.reddit.com"
        self.last_posts = set()  # ì¤‘ë³µ ë°©ì§€
        
    async def scan(self):
        """ì†Œì…œ ë¯¸ë””ì–´ íŠ¸ë Œë“œ ìŠ¤ìº”"""
        alerts = []
        
        try:
            # WallStreetBets ìŠ¤ìº”
            wsb_mentions = await self.scan_subreddit('wallstreetbets')
            
            # ê¸‰ì¦ ì¢…ëª© ì°¾ê¸°
            from config import Config
            for symbol, count in wsb_mentions.most_common(10):
                if count >= Config.REDDIT_MIN_MENTIONS:
                    
                    # ì‹¤ì œ ê°€ê²© ì •ë³´ëŠ” ë‚˜ì¤‘ì— ì±„ì›Œì§
                    alert = {
                        'symbol': symbol,
                        'price': 0,
                        'change_percent': 0,
                        'volume': 0,
                        'trigger_type': 'social_trend',
                        'trigger_reason': f'Reddit ê¸‰ë“± ì–¸ê¸‰ {count}íšŒ (1ì‹œê°„)'
                    }
                    
                    alerts.append(alert)
                    logger.info(f"ğŸ”¥ {symbol} trending on Reddit: {count} mentions")
            
        except Exception as e:
            logger.error(f"Social scan error: {e}")
        
        return alerts
    
    async def scan_subreddit(self, subreddit):
        """íŠ¹ì • ì„œë¸Œë ˆë”§ ìŠ¤ìº”"""
        mentions = Counter()
        
        try:
            # Reddit JSON API (ì¸ì¦ ë¶ˆí•„ìš”)
            url = f"{self.reddit_base}/r/{subreddit}/new.json"
            headers = {'User-Agent': 'StockScanner/1.0'}
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, timeout=10) as response:
                    if response.status != 200:
                        logger.error(f"Reddit API error: {response.status}")
                        return mentions
                    
                    data = await response.json()
                    posts = data.get('data', {}).get('children', [])
                    
                    cutoff_time = datetime.now() - timedelta(hours=1)
                    
                    for post_data in posts[:50]:  # ìµœê·¼ 50ê°œ
                        try:
                            post = post_data['data']
                            
                            # ì¤‘ë³µ ì²´í¬
                            post_id = post['id']
                            if post_id in self.last_posts:
                                continue
                            
                            # ì‹œê°„ ì²´í¬
                            post_time = datetime.fromtimestamp(post['created_utc'])
                            if post_time < cutoff_time:
                                continue
                            
                            # í‹°ì»¤ ì¶”ì¶œ
                            title = post.get('title', '')
                            selftext = post.get('selftext', '')
                            text = f"{title} {selftext}"
                            
                            tickers = self.extract_tickers(text)
                            mentions.update(tickers)
                            
                            self.last_posts.add(post_id)
                            
                        except Exception as e:
                            logger.error(f"Error processing post: {e}")
                            continue
                    
                    # ë©”ëª¨ë¦¬ ê´€ë¦¬
                    if len(self.last_posts) > 500:
                        self.last_posts.clear()
                    
        except asyncio.TimeoutError:
            logger.error(f"Timeout scanning r/{subreddit}")
        except Exception as e:
            logger.error(f"Error scanning r/{subreddit}: {e}")
        
        return mentions
    
    def extract_tickers(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ì£¼ì‹ í‹°ì»¤ ì¶”ì¶œ"""
        # $AAPL ë˜ëŠ” AAPL í˜•íƒœ
        pattern = r'\$?([A-Z]{2,5})\b'
        matches = re.findall(pattern, text.upper())
        
        # ì¼ë°˜ ì˜ì–´ ë‹¨ì–´ ì œì™¸
        exclude_words = {
            'THE', 'AND', 'OR', 'NOT', 'BUT', 'FOR', 'ARE', 'WAS', 'WERE',
            'YOLO', 'DD', 'TA', 'CEO', 'CFO', 'IPO', 'ATH', 'ATL',
            'MOON', 'HOLD', 'LONG', 'SHORT', 'CALL', 'PUT', 'BUY', 'SELL',
            'GOOD', 'BEST', 'HUGE', 'HUGE', 'FROM', 'THIS', 'THAT',
            'WHAT', 'WHEN', 'WHERE', 'WHO', 'WHY', 'HOW',
            'JUST', 'LIKE', 'MAKE', 'TIME', 'YEAR', 'WEEK', 'HAVE'
        }
        
        tickers = []
        for match in matches:
            if match not in exclude_words and 2 <= len(match) <= 5:
                tickers.append(match)
        
        return tickers