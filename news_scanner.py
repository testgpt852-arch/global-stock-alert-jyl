import aiohttp
import asyncio
import logging
import feedparser
from bs4 import BeautifulSoup
from config import Config

logger = logging.getLogger(__name__)

class NewsScanner:
    def __init__(self, api_key=None):
        self.api_key = api_key
        self.seen_news = set()
        
        self.sources = [
            # 1. [êµì²´] ì•¼í›„ íŒŒì´ë‚¸ìŠ¤ RSS (Business Wire í¬í•¨ ì „ ì„¸ê³„ ì†ë³´ ë¬´ì œí•œ ìˆ˜ì§‘)
            # ì°¨ë‹¨ ì—†ìŒ, ì§€ì—° ì‹œê°„ ê±°ì˜ ì—†ìŒ (ê°€ìž¥ í™•ì‹¤í•œ ë°©ë²•)
            {
                'name': 'Yahoo Finance',
                'type': 'yahoo_rss',
                'url': 'https://finance.yahoo.com/news/rssindex'
            },
            # 2. GlobeNewswire (ê³µì‹ RSS - ì•„ì£¼ ìž˜ ìž‘ë™ ì¤‘)
            {
                'name': 'GlobeNewswire',
                'type': 'direct_rss',
                'url': 'https://www.globenewswire.com/RssFeed'
            },
            # 3. PR Newswire (HTML í¬ë¡¤ë§ - ì•„ì£¼ ìž˜ ìž‘ë™ ì¤‘)
            {
                'name': 'PR Newswire',
                'type': 'html',
                'url': 'https://www.prnewswire.com/news-releases/news-releases-list/',
                'base_url': 'https://www.prnewswire.com'
            }
        ]

    async def scan(self):
        """ê¸€ë¡œë²Œ ë‰´ìŠ¤ í†µí•© ìŠ¤ìº”"""
        all_news = []
        tasks = []
        for source in self.sources:
            if source['type'] == 'yahoo_rss' or source['type'] == 'direct_rss':
                tasks.append(self._fetch_rss(source))
            else:
                tasks.append(self._fetch_html(source))
                
        results = await asyncio.gather(*tasks, return_exceptions=True)
        for result in results:
            if isinstance(result, list):
                all_news.extend(result)
                
        # [ì •ë ¬] ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬ (ì•¼í›„ì™€ ê¸€ë¡œë¸Œë‰´ìŠ¤ê°€ ì„žì—¬ë„ ìµœì‹ ì´ ìœ„ë¡œ ì˜¤ë„ë¡)
        # ë³´í†µ RSSëŠ” ìµœì‹ ìˆœì´ì§€ë§Œ, ì—¬ëŸ¬ ì†ŒìŠ¤ë¥¼ í•©ì¹˜ë¯€ë¡œ ë‹¤ì‹œ ì •ë ¬
        return sorted(all_news, key=lambda x: x.get('title', ''), reverse=True)

    async def _fetch_rss(self, source):
        """RSS íŒŒì‹± (ì•¼í›„ íŒŒì´ë‚¸ìŠ¤ & GlobeNewswire)"""
        news_items = []
        # ì•¼í›„ íŒŒì´ë‚¸ìŠ¤ëŠ” ë´‡ì„ ë§‰ì§€ ì•Šì§€ë§Œ, ì˜ˆì˜ìƒ í—¤ë” ì¶”ê°€
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/rss+xml, application/xml, text/xml, */*'
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(source['url'], headers=headers, timeout=10) as response:
                    if response.status != 200:
                        logger.error(f"{source['name']} RSS Error: {response.status}")
                        return news_items
                    
                    xml_content = await response.text()
                    feed = feedparser.parse(xml_content)
                    
                    if not feed.entries: return news_items

                    for entry in feed.entries[:15]:
                        try:
                            title = entry.title
                            link = entry.link
                            
                            # ì•¼í›„ íŒŒì´ë‚¸ìŠ¤ëŠ” ì£¼ì‹ í‹°ì»¤ë¥¼ RSSì— í¬í•¨í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ US ê¸°ë³¸ê°’
                            # (ì‹¤ì œ í˜¸ìž¬ íŒë…ì€ AIê°€ ì œëª©/ë³¸ë¬¸ìœ¼ë¡œ í•˜ë¯€ë¡œ ë¬¸ì œì—†ìŒ)
                            symbol = "US"
                            
                            self._add_if_valid(news_items, title, link, symbol, source['name'])
                        except: continue
                        
        except Exception as e:
            logger.error(f"{source['name']} RSS error: {e}")
            
        return news_items

    async def _fetch_html(self, source):
        """HTML í¬ë¡¤ë§ (PR Newswire)"""
        news_items = []
        headers = {'User-Agent': 'Mozilla/5.0'}
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(source['url'], headers=headers, timeout=10) as response:
                    if response.status != 200: return news_items
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    articles = soup.select('.card-list .card')[:15]
                    for article in articles:
                        try:
                            title_elem = article.select_one('h3')
                            if not title_elem: continue
                            a_tag = title_elem.find('a')
                            if a_tag:
                                title = a_tag.get_text(strip=True)
                                link = a_tag['href']
                            else:
                                title = title_elem.get_text(strip=True)
                                link = article.find('a')['href']
                            
                            if link and not link.startswith('http'): 
                                link = source['base_url'] + link
                            self._add_if_valid(news_items, title, link, "US", source['name'])
                        except: continue
        except Exception: pass
        return news_items

    def _add_if_valid(self, news_list, title, url, symbol, source_name):
        if url in self.seen_news: return
        
        # ì¤‘ë³µ ë‰´ìŠ¤ ë°©ì§€ (ì•¼í›„ê°€ GlobeNewswire ê¸°ì‚¬ë¥¼ ë˜ ê°€ì ¸ì˜¬ ìˆ˜ë„ ìžˆìŒ)
        # urlì´ ë‹¤ë¥¼ ìˆ˜ ìžˆìœ¼ë¯€ë¡œ ì œëª©ìœ¼ë¡œë„ ëŠìŠ¨í•œ ì¤‘ë³µ ì²´í¬ ê°€ëŠ¥í•˜ì§€ë§Œ,
        # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ URL ê¸°ì¤€ìœ¼ë¡œ ì‹¬í”Œí•˜ê²Œ ê°
        
        is_positive = any(k in title.lower() for k in Config.POSITIVE_KEYWORDS)
        is_negative = any(k in title.lower() for k in Config.NEGATIVE_KEYWORDS)
        
        if is_positive and not is_negative:
            self.seen_news.add(url)
            news_list.append({
                'symbol': symbol,
                'title': title,
                'url': url,
                'trigger_type': 'news_sentiment',
                'trigger_reason': f'ðŸ“° {source_name} í˜¸ìž¬ ë°œê²¬',
                'source': source_name
            })
            if len(self.seen_news) > 1000: self.seen_news.clear()